{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#169; CentraleSup√©lec. For any questions or comments, contact <frederic.pennerath@centralesupelec.fr>.**\n",
    "\n",
    "\n",
    "# The Tweetoscope problem:<br>Predicting popularity of tweets using Hawkes processes\n",
    "\n",
    "Here we want to predict how popular will be a tweet just by observing the earlier retweets following it.\n",
    "This study is meant to be instructive and to illustrate how to face a true data science problem properly.\n",
    "\n",
    "## Code of conduct of data scientists\n",
    "\n",
    "So if you don't want to end desesperate and start to pray the antique gods, practice some voodoo ceremony, and finally smash your computer with a baseball bat, you would better listen to the few preliminary advices:\n",
    "\n",
    "* **Method** and **rigor**  are two required skills for a data scientist: do not think your computer is clever so that it will fix your mistakes for you. Always double check everything at every step before jumping to the next one.\n",
    "* **Common sense** and **persistence** are also very important: don't be naive and overly optimistic.There is no magic in statistics and machine learning. Do not expect miracles when facing a problem that is complex in nature, impredictable or poorly instrumented (and here our problem is all three at once). So don't be disappointed if performances are low and never give up: continue working hard in order to provide a best effort solution.\n",
    "* **Perspicacity** and **curiosity**: nothing worse for a being endowed with intelligence, than applying some black box model without understanding what's happening under the cover. Using your brain always pays off. Use your knowledge in statistics to get intuition about underlying issues. If you feel you miss something, try things, change experiment settings, etc, just to have a better sense of how this works.\n",
    "\n",
    "\n",
    "## Vocabulary and reference\n",
    "\n",
    "So before getting your hands dirty, let's just formalize the problem and introduce some common vocabulary.\n",
    "\n",
    "Given an original tweet, its *cascade* is the collection of retweets it generates, where a retweet is abstracted as a *marked event*, i.e a couple (t,m) where:\n",
    "* t is the timestamp of the event/retweet. Here we will asume time start at 0, that is, the original tweet of a cascade is posted at $t=0$.\n",
    "* m is the mark of the event, also *called magnitude*, defined here as the number of followers the author of the retweet has.\n",
    "\n",
    "The *popularity of a tweet* is measured as the *size of its cascade*, defined as the number of retweets the cascade contains (including the initial tweet).\n",
    "Hereafter we consider the *problem of predicting the popularity of a tweet* after observing the earlier part of its cascade, during a given *observation window* of length $T_{obs}$ of typically 10 to 30 minutes starting from the initial tweet.\n",
    "We base our work on marked Hawkes processes (introduced during the lectures) as described in the articles:\n",
    "\n",
    "> [**SEISMIC: A Self-Exciting Point Process Model for Predicting Tweet Popularity**](http://snap.stanford.edu/seismic/seismic.pdf)  \n",
    "> *Q. Zhao, M. Erdogdu, H. He, A. Rajaraman, J. Leskovec*  \n",
    "> in **KDD'15:** *Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2015*\n",
    "\n",
    "\n",
    "> [**Feature Driven and Point Process Approaches for Popularity Prediction**](https://arxiv.org/abs/1608.04862)    \n",
    "> *Swapnil Mishra, Marian-Andrei Rizoiu, Lexing Xie*  \n",
    "> in **CIKM '16:** *Proceedings of the 25th ACM International on Conference on Information and Knowledge Management*\n",
    "\n",
    "## Let's make it simple\n",
    "\n",
    "The proposed models in the articles are however relatively complex to implement and to tune.\n",
    "We here propose the simplest possible model based on Hawkes process so that you can implement and understand all the method by yourself (derivation of mathematical expressions, simulation, estimation of parameters by optimization, prediction, etc), at the sacrifice of the model performance.\n",
    "\n",
    "**Therefore we expect from you to make the method operational, not to reach the same level of performance as the methods of the state of the art (obviously it won't).**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of model\n",
    "\n",
    "Compared to the aferomentioned articles, we make a simpler choice for our model:\n",
    "We assume the point process of retweets in a cascade is a Hawkes process with an exponential kernel, to keep derivations simple. During the lectures and class tutorial, we started to derive some useful expressions in this context. There are recalled hereafter.\n",
    "\n",
    "Let represent a cascade by a point process $\\{T_i \\in \\mathbb{R}^+ : i \\in \\mathbb{N}^*\\}$ on the real half line. Assuming the initial tweet of the cascade is posted at $t = 0$, $T_1 = 0$ represents the initial tweet and $\\{ T_i : i \\geq 2\\}$ are the subsequent retweet timestamps. \n",
    "\n",
    "Every time point $t_i$ is complemented by a useful piece of information, which is the number $m_i$ of followers of the author of retweet $t_i$. This number $m_i$ is hereafter referred as the *magnitude* of point $t_i$ (as a reference to earthquake replicas since earthquake modelling has been the initial application of Hawkes process). We formalize this by a so called *marked process* $\\{(T_i, M_i) \\in \\mathbb{R}^ +\\times \\mathbb{R}^+ : i \\in \\mathbb{N}^*\\}$, i.e where every point $t_i$ is *marked* by some magnitude $m_i$.\n",
    "\n",
    "Given some time $t$, we denote $\\mathcal{H}_{t}$ the history at time $t$, i.e the set $\\mathcal{H}_t = \\{(t_i,m_i) \\}_{1 \\leq i \\leq n} : t_i \\leq t\\}$ of marked points observed up to time $t$.\n",
    "Let denote $\\{N_t : t \\in \\mathbb{R}^+ \\}$ the equivalent counting process of $\\{T_i\\}$ equal to the number of (re)tweets at time $t$ for a given cascade. Its *conditional intensity* $\\lambda^*(t)$ at time $t$ is defined as:\n",
    "$$\\lambda^*(t) = p \\, \\sum_{i=1}^n m_i \\, \\phi(t-t_i)$$\n",
    "where:\n",
    "* Function $\\phi$ specifies the *shape of the kernel*. It is normalized such that $\\int_0^{+\\infty} \\phi(t) \\, dt = 1$ (i.e $\\phi$ can be seen as a density but it is not, it is an intensity). References say that the kernel shape for tweets typically follows a *power-law density*. However for sake of simplicity, we prefer an *exponential kernel* parameterized by *responsiveness* $\\beta > 0$ whose density is\n",
    "$$\\phi(t) = \\beta \\, \\exp(-\\beta \\, t) \\, \\mathbb{1}_{]0,+\\infty[}(t)\\quad.$$\n",
    "* Kernel $(t, m_i) \\mapsto m_i \\, \\phi(t)$ is the intensity of retweets generated by an initial (re)tweet posted at time 0, if **all $m_i$ followers retweet them**. Can you see why? Here there is a flaw in the reasoning, that makes Hawkes processes partially inadequate for the problem. Can you see it?).\n",
    "* Factor $p$ is the probability (on average) that a follower retweets the current tweet. Therefore $p$ quantifies the virality of the message content.\n",
    "\n",
    "**Note:** notation *$\\mathbb{1}_A$ refers hereafter to the characteristic function of set $A$: $\\mathbb{1}_A(x) = 1$ if $x \\in A$, $0$ otherwise.*\n",
    "\n",
    "As seen during the lectures, specifying function $\\lambda^*(t)$ is sufficient to generate time points $t_i$.\n",
    "However to generate marked points $(t_i,m_i)$, we also need to know how magnitudes are distributed.\n",
    "Again to keep it simple, we just follow a very coarse assumption given in [Mishra et al. 16]. In this work, they observed that the numbers of followers of the whole population of Twitter users follow a negative **power-law distribution** whose density is characterized by two parameters, the negative power $\\alpha > 2$ and the minimal magnitude $\\mu \\geq 1$:\n",
    "$$\n",
    "f_M(m) = \\frac{\\alpha-1}{\\mu} \\, \\left(\\frac{m}{\\mu}\\right)^{-\\alpha}\\, \\mathbb{1}_{[\\mu,+\\infty[}(m) \\quad .\n",
    "$$\n",
    "The authors then chosed $\\mu = 1$ and found $\\alpha = 2.016$ by fitting the theoretical distribution with a very large number of tweet samples.\n",
    "\n",
    "> **Question:** Why is this very naive?\n",
    "\n",
    "In summary, every cascade is thus only characterized by its two own parameters $(p,\\beta)$ and one global constant parameter $\\alpha$.\n",
    "\n",
    "So let's start, by loading required modules and apply some default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "This is very naive as it amounts to consider magnitude $M_i$ independent with $i$ (i.e the rank of the retweet).\n",
    "However in practice (and this can be observed on the provided data), the observed magnitudes $(m_i)$ tend to decrease with $i$ as the retweet cascade is initially launched and spread by VIPs with very large number of followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 2, suppress=True) # Default precision when displaying numpy arrays \n",
    "mpl.rcParams['figure.figsize'] = [ 16, 8 ] # Default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work first on simulated data\n",
    "\n",
    "The purpose of this labwork is \n",
    "1. first to estimate the parameters of a cascade given its samples observed during an observation window \n",
    "2. then to use these estimated parameters to predict the final cascade size.\n",
    "\n",
    "But... Getting an estimation algorithm working might sometimes be tricky.\n",
    "A good practice to test and debug such an algorithm is to work first on simulated data before applying it to real data.\n",
    "The first debugging stage consists in generating data from some known instance of the model (for some arbitrarily chosen parameter values) and then checking that the estimation algorithm is able to recover the right parameter values from these samples (for a sufficiently large number of them).\n",
    "This allows to fix implementation problems intrinsic to the method (bugs, high variance, choice for hyper parameters, etc).\n",
    "Only once the algorithm works well on simulated data, we can be confident in our algorithm and apply it to real data. We then know the error increase is mostly due to discrepancies between the model and the real data, not about the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation algorithm\n",
    "\n",
    "The first step is to implement an efficient algorithm for simulating a marked Hawkes process.\n",
    "To simulate point processes from their conditional intensity function $t \\mapsto \\lambda^*(t)$, there are two general algorithms: one is based on *inverting the cumulative intensity* (see lecture), one based on rejection sampling (a notion that will be studied later in the course *algorithms for data science*) called the *thinning algorithm*.\n",
    "In case of Hawkes processes, computing the intensity at time $t$ depends on the number $N_t$ of points observed before $t$. To generate one point, we thus need to loop at least $\\Theta(N_t)$ times to evaluate its cumulative intensity.\n",
    "Iterating on points, we get algorithms with at least a quadratic complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### An efficient algorithm for exponential kernels\n",
    "\n",
    "However choosing an exponential kernel allows to derive an efficient linear algorithm as shown below.\n",
    "Assuming an exponential kernel \n",
    "$$\\phi(d) = \\beta \\, \\exp(-\\beta \\, d) \\, \\mathbb{1}_{> 0}(d)$$\n",
    "and history $\\mathcal{H}_{t_k}$ at some time point $t_k$ the conditional intensity $\\lambda^*(t)$ for $t > t_k$ for the next point $T_{k+1}$ can be rewritten as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lambda^*(t) & = \\sum_{i=1}^{k} p \\, m_i \\, \\beta \\, \\exp(-\\beta \\, (t - t_i))\\\\\n",
    " & = \\sum_{i=1}^{k} p \\, m_i \\, \\beta \\, \\exp(-\\beta \\, (t_k - t_i)) \\, \\exp(-\\beta \\, (t - t_k))\\\\\n",
    " & = \\lambda^*(t_k) \\, \\exp(- \\beta \\, (t - t_k))\n",
    "\\end{align}\n",
    "$$\n",
    "where term $\\lambda^*(t_k)$ can be updated in constant time, every time $k$ is increased (i.e a new time point is sampled) as detailed below:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lambda^*(t_k) & = \\sum_{i=1}^{k} p \\, m_i \\, \\beta \\, \\exp(-\\beta \\, (t_k - t_i))\\\\\n",
    "& = \\lambda^*(t_{k-1}) \\, \\exp(-\\beta \\, (t_k - t_{k-1})) + p \\, m_k \\, \\beta \\quad .\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now the remaining question is how to sample the next point $T_{k+1}$. This can be done by inverting the *cumulative intensity function* $t \\mapsto \\Lambda^*(t)$ (see lecture). \n",
    "Let $t$ be the value of the next point $T_{k+1}$. Then\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Lambda^*(t) & =  \\int_{t_k}^{t} \\lambda^*(\\tau) \\, d\\tau  \\\\\n",
    "& = \\int_{t_k}^{t} \\lambda^*(t_k) \\, \\exp(-\\beta \\, (\\tau - t_k)) \\, d\\tau \\\\\n",
    "& = \\frac{\\lambda^*(t_k)}{\\beta} \\, \\left(1 - \\exp(-\\beta  \\, (t - t_k)) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "According to the theorem seen during the lectures, we know that:\n",
    "$$\\Lambda^*(T_{k+1}) \\sim Exp(1)$$\n",
    "\n",
    "Let $V \\sim Exp(1)$, then\n",
    "$$\n",
    "\\begin{align}\n",
    "T_{k+1} & = {\\Lambda^*}^{-1}(V)\\\\\n",
    "& = t_k -\\frac{1}{\\beta} \\, \\log \\left(1 - \\frac{\\beta}{\\lambda^*(t_k)} \\, V \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that if $V \\geq \\frac{\\lambda^*(t_k)}{\\beta}$ however, there is no more time point and the process is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation and test\n",
    "It is time to test this simulation algorithm. One provides the implementation for the case of a simple Hawkes process or equivalenty a marked Hawkes process such that all magnitudes $m_i$ are all equal to a constant $m$ so that, by setting $\\kappa = p \\, m$, we have:\n",
    "$$\\lambda^*(t) = \\sum_{i=1}^n \\kappa\\, \\phi(t-t_i)$$\n",
    "For more flexibility in the simulation, we assume the intensity due to the initial tweet is different from $\\kappa$, equal to some other constant $\\lambda_0$.\n",
    "\n",
    "Please read and understand function `simulate_exp_hawkes_process` with respect to the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_exp_hawkes_process(params, lambda0, max_size=2000):\n",
    "    \"\"\"\n",
    "    Returns a 2D-array containing marked time points simulated from an exponential Hawkes process\n",
    "    \n",
    "    params   -- parameter tuple (kappa,beta) of the generating process\n",
    "    lambda0  -- initial intensity at t = 0.\n",
    "    max_size -- maximal authorized size of the cascade\n",
    "    \"\"\"\n",
    "    \n",
    "    kappa, beta = params    \n",
    "    \n",
    "    # The result is just a 1D array\n",
    "    # However for further compatibility, we reserve a second colum for the magnitude of every point.\n",
    "    # Every row thus describes a marked timepoint (ti, mi) with a magnitude set arbitrarily to one.\n",
    "\n",
    "    # Create an unitialized array for optimization purpose (only memory allocation)\n",
    "    T = np.empty((max_size,2))\n",
    "    \n",
    "    # Set magnitudes to 1.\n",
    "    T[:,1] = 1.\n",
    "    \n",
    "    intensity = beta * lambda0\n",
    "    t = 0.\n",
    "    \n",
    "    # Main loop\n",
    "    for i in range(max_size):\n",
    "        # Save the current point before generating the next one.\n",
    "        T[i,0] = t\n",
    "        \n",
    "        # Sample inter-event time v from a homogeneous Poisson process\n",
    "        u = np.random.uniform()\n",
    "        v = -np.log(u)\n",
    "        \n",
    "        # Apply the inversion equation\n",
    "        w = 1. - beta / intensity * v\n",
    "        # Tests if process stops generating points.\n",
    "        if w <= 0.:\n",
    "            # Shrink T to remove unused rows\n",
    "            T = T[:i,:]\n",
    "            break\n",
    "            \n",
    "        # Otherwise computes the time jump dt and new time point t\n",
    "        dt = - np.log(w) / beta\n",
    "        t += dt\n",
    "        \n",
    "        # And update intensity accordingly\n",
    "        intensity = intensity * np.exp(-beta * dt) + beta * kappa       \n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide below a function `plot_cascade` for plotting the simulated cascade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cascade(cascade, Tmax = None):\n",
    "    plt.stem(cascade[:,0] / 60., cascade[:,1], use_line_collection=True)\n",
    "    plt.yscale('log')\n",
    "    if Tmax is not None:\n",
    "        plt.xlim(None, Tmax/60)\n",
    "    plt.xlabel('time (min)')\n",
    "    plt.ylabel('magnitude (log)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Setting for instance $\\beta = 1/3600$ (a time constant of 1 hour), simulate the process for different values of $\\kappa$ and $\\lambda_0$ and observe its plot.\n",
    "> In particular test for $\\kappa = 0.1$ and $\\lambda_0 = 100$, you should observe some clustering effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = (0.1, 1/3600.)\n",
    "lambda0 = 100\n",
    "cascade = simulate_exp_hawkes_process(params, lambda0, max_size=10000)\n",
    "\n",
    "print(f\"{len(cascade)} points generated\")\n",
    "plot_cascade(cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting processes\n",
    "\n",
    "*Counting processes* are an equivalent representation of *point processes*.  \n",
    "As a reminder, counting process $\\{ N_t \\in \\mathbb{N} | t \\in \\mathbb{R}^+ \\}$ mapped to point process $\\{ P_n \\in \\mathbb{R}^+ | n \\in \\mathbb{N}^*\\}$ is defined as:\n",
    "\n",
    "$$ N_t = \\left| \\, P_i \\leq t \\, | \\, i \\in \\mathbb{N}^* \\right| $$\n",
    "\n",
    "> **Question**:  \n",
    "> Implement function ``counting_process`` as documented and plot the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_Tmax = 1.1\n",
    "\n",
    "def counting_process(cascade, T = None):\n",
    "    \"\"\"\n",
    "    Returns a 2D-array N such that N(:,0) contains time samples t and N(:,1) contains images by point process N(t)\n",
    "    \n",
    "    cascade -- 2D-array containing samples of the point process as returned by simulate_exp_hawkes_process\n",
    "    T       -- 1D array containing time samples whose value N(t) has to be computed (if None defines T to cover the full cascade)\n",
    "    \"\"\"\n",
    "    \n",
    "    tks = cascade[:,0]\n",
    "    if T is None:\n",
    "        Tmax = tks[-1] * coef_Tmax\n",
    "        T = np.linspace(0,Tmax)\n",
    "    N = np.zeros((len(T),2))\n",
    "    N[:,0] = T\n",
    "    for tk in tks:\n",
    "        N[T >= tk,1] += 1\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = counting_process(cascade)\n",
    "plot_cascade(cascade)\n",
    "plt.plot(N[:,0]/60, N[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation to marked exponential Hawkes processes\n",
    "\n",
    "Now we consider the \"marked\" case where\n",
    "$$t > t_n \\implies \\lambda^*(t) = p \\, \\sum_{i=1}^n m_i \\, \\beta \\, \\exp\\left(\\beta(t-t_i)\\right)$$\n",
    "\n",
    "One recalls that event magnitudes $M_i$ are assumed to be i.i.d and drawn from a power law distribution of parameters $\\mu$ and $\\alpha$, whose density is\n",
    "$$\n",
    "f_M(m) = \\frac{\\alpha-1}{\\mu} \\, \\left(\\frac{m}{\\mu}\\right)^{-\\alpha}\\, \\mathbb{1}_{[\\mu,+\\infty[}(m) \\quad .\n",
    "$$\n",
    "We choose different values as in [Mishra et al. 16], for stability purpose: instead, we take $\\mu = 10$ and $\\alpha = 2.3$.\n",
    "\n",
    "Unfortunately numpy function ``np.random.power`` only draws samples from power-law with **positive** power parameter.\n",
    "One solution would be to draw samples $y$ from a $(+\\alpha,1)$- power law distribution (i.e with opposite & positive parameter alpha) and then take $m = \\frac{\\mu}{y}$ (why would this work?)\n",
    "\n",
    "Instead we propose to use this limitation as an excellent opportunity to apply the *inverse transform sampling* technique we saw during the lectures: this technique consists in drawing a uniform sample *u* in the interval $[0,1]$ and apply to $u$ the inverse function $F_M^{-1}(u)$ of the cumulative distribution function $F_M(m) = P(M \\leq m)$ of $M$. The resulting value $s$ is a sample of $M$.\n",
    "\n",
    "> **Question:**  \n",
    "> Implement the ``neg_power_law`` function using this technique (use ``np.random.uniform(size)`` for this purpose).\n",
    "> Check your function correctly works by comparing the histogram of many samples with the theoretical density function (you can use the provided code snippet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_power_law(alpha, mu, size=1):\n",
    "    \"\"\"\n",
    "    Returns a 1D-array of samples drawn from a negative power law distribution\n",
    "    \n",
    "    alpha -- power parameter of the power-law mark distribution\n",
    "    mu    -- min value parameter of the power-law mark distribution\n",
    "    size  -- number of samples\n",
    "    \"\"\"\n",
    "    \n",
    "    u = np.random.uniform(size=size)\n",
    "    X = mu * np.exp(np.log(u) / (1. - alpha))\n",
    "    if size==1:\n",
    "        return X[0]\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 10; alpha = 2.3; \n",
    "\n",
    "# Draw samples \n",
    "samples = neg_power_law(alpha, mu, size=100000)\n",
    "\n",
    "# Compare average with mean\n",
    "print(f\"sample average = {np.mean(samples):0.2f}, expected value = {mu * (alpha - 1)/(alpha - 2):0.2f}\")\n",
    "\n",
    "# Draw histogram\n",
    "max_mark = 100\n",
    "plt.hist(samples[samples < max_mark], range=(1,max_mark), bins=1000,density=True,log=True)\n",
    "\n",
    "# Compare with theoretical density\n",
    "m = np.linspace(mu,max_mark)\n",
    "plt.plot(m, (alpha - 1) / mu * (m / mu) ** (-alpha), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Fill in function `simulate_marked_exp_hawkes_process` below by adapting `simulate_exp_hawkes_process` so that it simulates on a marked Hawkes process parameterized by $(p, \\beta)$-exponential kernel and a $(\\mu,\\alpha)$-power law mark distribution. You will reuse your ``neg_power_law`` function.\n",
    ">Then simulate and plot the process for different parameter values.\n",
    "\n",
    "*Note:* *we will see later that the following condition must be verified for the process not to diverge*\n",
    " $$ n^* = p \\, \\mu \\, \\frac{\\alpha - 1}{\\alpha - 2} < 1$$\n",
    "> **Question:**  \n",
    "> Observe what happens when you take parameter values so that $n^*$ gets smaller or larger than $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_marked_exp_hawkes_process(params, m0, alpha, mu, max_size=10000):\n",
    "    \"\"\"\n",
    "    Returns a 2D-array whose rows contain marked time points simulated from an exponential Hawkes process\n",
    "    \n",
    "    params   -- parameter tuple (p,beta) of the generating process\n",
    "    m0       -- magnitude of the initial tweet at t = 0.\n",
    "    alpha    -- power parameter of the power-law mark distribution\n",
    "    mu       -- min value parameter of the power-law mark distribution\n",
    "    max_size -- maximal authorized size of the cascade\n",
    "    \"\"\"\n",
    "    \n",
    "    p, beta = params    \n",
    "    \n",
    "    # Every row contains a marked time point (ti,mi).\n",
    "    # Create an unitialized array for optimization purpose (only memory allocation)\n",
    "    T = np.empty((max_size,2),dtype=float)\n",
    "    \n",
    "    intensity = beta * p * m0\n",
    "    t, m = 0., m0\n",
    "    \n",
    "    # Main loop\n",
    "    for i in range(max_size):\n",
    "        # Save the current point before generating the next one.\n",
    "        T[i] = (t,m)\n",
    "        \n",
    "        # Sample inter-event time v from a homogeneous Poisson process\n",
    "        u = np.random.uniform()\n",
    "        v = -np.log(u)\n",
    "        \n",
    "        # Apply the inversion equation\n",
    "        w = 1. - beta / intensity * v\n",
    "        # Tests if process stops generating points.\n",
    "        if w <= 0.:\n",
    "            T = T[:i,:]\n",
    "            break\n",
    "            \n",
    "        # Otherwise computes the time jump dt and new time point t\n",
    "        dt = - np.log(w) / beta\n",
    "        t += dt\n",
    "        \n",
    "        # And update intensity accordingly\n",
    "        m = neg_power_law(alpha, mu)\n",
    "        lambda_plus = p * m\n",
    "        intensity = intensity * np.exp(-beta * dt) + beta * lambda_plus        \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p, beta = 0.025, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000\n",
    "n_star = p * mu * (alpha - 1) / (alpha - 2)\n",
    "print(f\"n_star = {n_star:.2f}\")\n",
    "cascade = simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu, max_size=10000)\n",
    "\n",
    "print(f\"{len(cascade)} points generated\")\n",
    "plot_cascade(cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing marked Hawkes processes\n",
    "\n",
    "To analyze the dynamics of a point process, it's first important to compute and plot its conditional intensity.\n",
    "We recall that\n",
    "$$\\lambda^*(t) = \\sum_{t_i < t} p \\, m_i \\, \\beta \\, \\exp(-\\beta \\,(t-t_i))$$\n",
    "\n",
    "> **Question:**  \n",
    "> Implement function `cond_intensity` to compute the intensity of a Hawkes process with exponential kernel given its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_intensity(params, history, T):\n",
    "    \"\"\"\n",
    "    Returns a numpy 2D-array containing the conditional intensity of an exponential Hawkes process\n",
    "    (first column is time, second is mapped intensity)\n",
    "    \n",
    "    params   -- parameter tuple (p,beta) of the Hawkes process\n",
    "    history  -- (n,2) numpy array containing marked time points (t_i,m_i)\n",
    "    T        -- 1D-array containing the input times for which the intensity must be computed\n",
    "    \"\"\"\n",
    "    \n",
    "    p, beta = params    \n",
    "    I = np.zeros((len(T),2))\n",
    "    I[:,0] = T\n",
    "                 \n",
    "    # For every marked point,\n",
    "    for ti,mi in history:\n",
    "        # Get all time indexes whose time is greater than ti\n",
    "        J = T >= ti\n",
    "        # Update the intensity for all times larger thanti\n",
    "        I[J,1] += mi * np.exp(-beta * (T[J]-ti))\n",
    "        \n",
    "    # Don't forget to multiply by p*beta\n",
    "    I[:,1] *= p * beta\n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Test by generating a cascade and then draw the resulting conditional intensity using the already implemented `draw_intensity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_intensity(params, history, Tmax = None, label = \"\"):\n",
    "    \"\"\"\n",
    "    Draws an intensity plot along the history\n",
    "    \n",
    "    params   -- parameter tuple (p,beta) of the Hawkes process\n",
    "    history  -- (n,2) numpy array containing marked time points (t_i,m_i)\n",
    "    Tmax     -- upper bound of the plot interval\n",
    "    label    -- legend label\n",
    "    \"\"\"\n",
    "\n",
    "    if Tmax is None:\n",
    "        Tmax = history[-1,0] * coef_Tmax\n",
    "    T = np.linspace(-10., Tmax, 1000)\n",
    "    I = cond_intensity(params, history, T)\n",
    "    plt.plot(I[:,0] / 60., I[:,1] , label = label)\n",
    "    plt.plot(history[:,0]/60, np.zeros(len(history)),'o', color='red')\n",
    "    plt.title('Process intensity')\n",
    "    plt.xlabel('Time (min)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_intensity((p,beta), cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the average number of generated points of a process given its conditional intensity, we just need to compute the cumulative intensity function\n",
    "$$\\Lambda(t) = \\int_0^t \\lambda^*(\\tau) \\, d\\tau$$\n",
    "Then the average number of points in an interval $[a,b]$ is just\n",
    "$$\\mathbb{E}(N(b) - N(a)) = \\int_a^b \\lambda^*(\\tau) \\, d\\tau = \\Lambda(b) - \\Lambda(a)$$\n",
    "\n",
    "> **Question:**  \n",
    "> Complete function ``cumul_intensity`` to compute the cumulative intensity function  \n",
    "\n",
    "**Indication:** *to compute the integral, you can either take a general approach (recommended) using function ``scipy.integrate.cumtrapz``, or you can compute the exact value of the integral specifically for exponential kernels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "def cumul_intensity(cond_intensity):\n",
    "    \"\"\"\n",
    "    Returns a 2D array containing the cumulative intensity such that first column is time\n",
    "    and second is mapped cumulative intensity up to given time.\n",
    " \n",
    "    cond_intensity -- 2D-array as returned by cond_intensity function\n",
    "    \"\"\"\n",
    "    \n",
    "    T = cond_intensity[:,0]\n",
    "    I = cond_intensity[:,1]\n",
    "    \n",
    "    C = np.empty_like(cond_intensity)\n",
    "    C[:,0] = T\n",
    "    C[:,1] = integrate.cumtrapz(I, T, initial=0)\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Check your function correctly works by drawing on the same plot the cumulative intensity (using the provided ``draw_cumul_intensity`` function) and the real counts (i.e given by computing the counting process N).\n",
    "> You should observe that the cumulative intensity is more or less the counting process (some gaps can occur), but looks the same locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cumul_intensity(params, history, Tmax=None, label=\"\"):\n",
    "    if Tmax is None:\n",
    "        Tmax = history[-1,0] * coef_Tmax\n",
    "    T = np.linspace(-10., Tmax, 1000)\n",
    "    I = cond_intensity(params, history, T)\n",
    "    cumul = cumul_intensity(I)\n",
    "    plt.plot(cumul[:,0]/60., cumul[:,1], label=label)\n",
    "    plt.xlabel('Time (min)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cumul_intensity((p,beta), cascade, label='$\\Lambda(t)$')\n",
    "N = counting_process(cascade)\n",
    "plt.plot(N[:,0]/60, N[:,1], label='N(t)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "We now look at the problem of predicting at current time $t$ the total size $N_\\infty(t)$ of a cascade given the parameters $(p,\\beta)$ of the generating process and its history $\\mathcal{H}_t = \\{(m_1,t_1) \\dots (m_n,t_n), T_{n+1} > t\\}$, that is\n",
    "$$\n",
    "N_\\infty(t) = \\lim_{\\tau \\rightarrow +\\infty} \\mathbb{E}\\left( N_{\\tau}\\, |\\, \\mathcal{H}_t \\right) \\quad .\n",
    "$$\n",
    "Now let's denote $G_k$ the number of points (i.e retweets) of the $k^{th}$ next generation:\n",
    "* For $k=1$, the first generation are points generated by the $n$ past points observed in $\\mathcal{H}_t$,\n",
    "* For $k>1$, the $k^{th}$ generation are those generated by a point of the previous $(k-1)^{th}$ generation.\n",
    "\n",
    "Then the expected total number of points is clearly:\n",
    "$$\n",
    "N_\\infty(t) = n + \\mathbb{E}\\left(G_1\\right) + \\mathbb{E}\\left(G_2\\right) +  \\mathbb{E}\\left(G_3\\right) \\dots\n",
    "$$\n",
    "\n",
    "What is the relation between $\\mathbb{E}\\left(G_{k+1}\\right)$ and $\\mathbb{E}\\left(G_k\\right)$ for $k\\geq 1$ ?\n",
    "\n",
    "To answer this question, let consider point (i.e retweet) number $i$ of the $k^{th}$ generation and let denote $N^{(k)}_i$ the number of points of the ${k+1}^{th}$ generation whose parent is $i$. Then\n",
    "$$\n",
    "\\mathbb{E}\\left(G_{k+1} \\, |\\, G_k = g \\right) = \\mathbb{E}\\left(\\sum_{i=1}^{g} N^{(k)}_i \\, \\middle|\\, G_k = g \\right) = \\sum_{i=1}^{g} \\mathbb{E}\\left( N^{(k)}_i \\right)\n",
    "$$\n",
    "Assuming magnitudes $M^{(k)}_i$ of points are independent and identically distributed (this is a very coarse hypothesis, why?), we get:\n",
    "$$\n",
    "\\mathbb{E}\\left( N^{(k)}_i \\right) = \\mathbb{E}\\left( N^{(k)}_i \\, |\\, M_i \\right) = \\int_1^{+\\infty} \\mathbb{E}\\left( N^{(k)} \\, |\\, M = m\\right)  \\, P_M(m) \\, dm\n",
    "$$\n",
    "Quantity $\\mathbb{E}\\left( N^{(k)} \\,| \\,M = m\\right)$ is the average number of points generated by one given point of generation $k$ and magnitude $m$. Its value is given by integrating the partial intensity due to this single point. Assuming the point occurs at $t=t_i$, we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left( N^{(k)} \\, |\\, M = m\\right)& = \\int_{t_i}^{+\\infty} p \\, m \\, \\phi(t-t_i) \\, dt\\\\\n",
    "& = p \\,m\n",
    "\\end{align}\n",
    "$$\n",
    "Replacing,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left( N^{(k)}_i \\right) & = \\int_1^{+\\infty} p \\, m \\, P_M(m) \\, dm\\\\\n",
    "& = p \\, \\mathbb{E}\\left(M\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "So\n",
    "$$\n",
    "\\mathbb{E}\\left(G_{k+1} \\, |\\, G_k = g\\right) = p \\, \\mathbb{E}\\left(M\\right) \\, g\n",
    "$$\n",
    "Finally\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left(G_{k+1}\\right) & = \\int_0^{+\\infty} \\mathbb{E}\\left(G_{k+1} \\, |\\, G_k = g\\right) \\, P_{G_k}(g) \\, dg \\\\\n",
    "& = \\int_0^{+\\infty} p \\, \\mathbb{E}\\left(M\\right) \\, g \\, P_{G_k}(g) \\, dg \\\\\n",
    "& = p \\, \\mathbb{E}\\left(M\\right) \\, \\mathbb{E}\\left(G_k\\right) \\\\\n",
    "& = n^* \\, \\mathbb{E}\\left(G_k\\right) \\text{ with } n^* = p \\, \\mathbb{E}\\left(M\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore $(\\mathbb{E}\\left(G_k\\right))_k$ is a geometric sequence and\n",
    "$$\n",
    "\\mathbb{E}\\left(G_k\\right) = {\\left(n^*\\right)}^{k-1} \\, \\mathbb{E}\\left(G_1\\right)\n",
    "$$\n",
    "\n",
    "The number denoted $n^*$ is called the **branching factor** of the self-exciting process. \n",
    "It is the number of points generated on average by a single point. As we can see, the global process behaves as if all points had a constant branching factor.\n",
    "\n",
    "**Note:** *this number is at the heart of the very hot COVID-19 topic: in the field of epidemiology, the branching factor is also called the **reproduction number $R$**, defined as the average number of people that catch the virus from one single person.*\n",
    "\n",
    "If $n^* \\geq 1$, obviously the sequence $G_k$ is positive non decreasing so that $N_\\infty(t) = +\\infty$.\n",
    "Otherwise the expected total number of points is:\n",
    "$$\n",
    "\\begin{align}\n",
    "N_\\infty(t) & = n + G_1 + G_2 + G_3 \\dots \\\\\n",
    "& = n + G_1 + n^* \\, G_1 + {n^*}^2 \\, G_1 + \\dots \\\\\n",
    "& = n + \\frac{G_1}{1 - n^*}\\\\\n",
    "& = n + \\frac{G_1}{1 - p \\, \\mathbb{E}\\left(M\\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "The two remaining unknowns are thus $G_1$ and $\\mathbb{E}\\left(M\\right)$.\n",
    "The latter is easy to compute:\n",
    "$$\n",
    "\\mathbb{E}\\left(M\\right)  = \\int_\\mu^{+\\infty} \\frac{\\alpha - 1}{\\mu} \\, \\left(\\frac{m}{\\mu}\\right)^{-\\alpha}\\, m \\, dm = \\frac{\\alpha - 1}{\\alpha - 2}\\, \\mu\n",
    "$$\n",
    "Since all cascades are assumed to be finite, the constraint $n^* < 1$ enforces the condition\n",
    "$$\n",
    "p \\, \\mu < \\frac{\\alpha - 2}{\\alpha - 1} \\quad .\n",
    "$$\n",
    "The value of $G_1$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "G_1 & = \\Lambda(+\\infty) - \\Lambda(t) = \\int_t^{+\\infty} \\lambda^*(t) \\, dt \\\\\n",
    "& = \\int_t^{+\\infty}  \\sum_{i=1}^n p \\, m_i \\, \\beta \\, \\exp(- \\beta \\, (t - t_i)) \\, dt \\\\\n",
    "& = p \\, \\sum_{i=1}^n \\, m_i \\, \\exp(- \\beta \\, (t - t_i)) \\quad .\n",
    "\\end{align}\n",
    "$$\n",
    "To summarize,\n",
    "$$\n",
    "N_\\infty(t) = n + \\frac{p \\, \\sum_{i=1}^n \\, m_i \\, \\exp(- \\beta \\, (t - t_i))}{1 - n^*} \\text{ with } n^* = p \\, \\mu \\, \\frac{\\alpha - 1}{\\alpha - 2} < 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Complete function ``prediction()`` that predicts given the parameters of the process and its history, the total size of a cascade for one single given time $t$.\n",
    "\n",
    "**Warning: at time $t$, the sum in expression of $G_1$ is computed only on points $(t_i,m_i)$ such that $t_i < t$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(params, history, alpha, mu, t):\n",
    "    \"\"\"\n",
    "    Returns the expected total numbers of points for a set of time points\n",
    "    \n",
    "    params   -- parameter tuple (p,beta) of the Hawkes process\n",
    "    history  -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    alpha    -- power parameter of the power-law mark distribution\n",
    "    mu       -- min value parameter of the power-law mark distribution\n",
    "    t        -- current time (i.e end of observation window)\n",
    "    \"\"\"\n",
    "\n",
    "    p,beta = params\n",
    "    \n",
    "    tis = history[:,0]\n",
    "   \n",
    "    EM = mu * (alpha - 1) / (alpha - 2)\n",
    "    n_star = p * EM\n",
    "    if n_star >= 1:\n",
    "        raise Exception(f\"Branching factor {n_star:.2f} greater than one\")\n",
    "    n = len(history)\n",
    "\n",
    "    I = history[:,0] < t\n",
    "    tis = history[I,0]\n",
    "    mis = history[I,1]\n",
    "    G1 = p * np.sum(mis * np.exp(-beta * (t - tis)))\n",
    "    Ntot = n + G1 / (1. - n_star)\n",
    "    return Ntot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> As a facility, complete function  ``predictions()`` that predicts given the parameters of the process and its history, the total size of a cascade for a set of current times (provided by a 1D-array $T$).  \n",
    "\n",
    "*Indication: the code provided for ``predictions()`` in the correction is optimized but a naive implementation is fine: just compute in one internal function the prediction for one single time according to the given expression above, and then call this function for every time in $T$.*\n",
    "\n",
    "**Warning: at time $t$, the sum in expression of $G_1$ is computed only on points $(t_i,m_i)$ such that $t_i < t$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(params, history, alpha, mu, T = None):\n",
    "    \"\"\"\n",
    "    Returns the expected total numbers of points for a set of time points\n",
    "    \n",
    "    params   -- parameter tuple (p,beta) of the Hawkes process\n",
    "    history  -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    alpha    -- power parameter of the power-law mark distribution\n",
    "    mu       -- min value parameter of the power-law mark distribution\n",
    "    T        -- 1D-array of times (i.e ends of observation window)\n",
    "    \"\"\"\n",
    "\n",
    "    p,beta = params\n",
    "    \n",
    "    tis = history[:,0]\n",
    "    if T is None:\n",
    "        T = np.linspace(60,tis[-1],1000)\n",
    "\n",
    "    N = np.zeros((len(T),2))\n",
    "    N[:,0] = T\n",
    "    \n",
    "    EM = mu * (alpha - 1) / (alpha - 2)\n",
    "    n_star = p * EM\n",
    "    if n_star >= 1:\n",
    "        raise Exception(f\"Branching factor {n_star:.2f} greater than one\")\n",
    "\n",
    "    Si, ti_prev, i = 0., 0., 0\n",
    "    \n",
    "    for j,t in enumerate(T):\n",
    "        for (ti,mi) in history[i:]:\n",
    "            if ti >= t:\n",
    "                break\n",
    "            else:\n",
    "                Si = Si * np.exp(-beta * (ti - ti_prev)) + mi\n",
    "                ti_prev = ti\n",
    "                i += 1\n",
    "\n",
    "        n = i + 1\n",
    "        G1 = p * Si * np.exp(-beta * (t - ti_prev))\n",
    "        N[j,1] = n + G1 / (1. - n_star)\n",
    "    return N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Test your prediction functions by generating a cascade from a process with known parameters.\n",
    "> Then plot the time evolution of the size prediction along with the counting process.\n",
    "> Test for the different parameters and see how predictions vary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.02, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "\n",
    "EM = mu * (alpha-1)/(alpha-2)\n",
    "print(f\"Branching factor n* = {p * EM:.2f}\")\n",
    "if p * EM >= 1:\n",
    "    raise Exception(f\"p must be less than {1/EM:.2f}\")\n",
    "\n",
    "cascade = simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu)\n",
    "\n",
    "print(f\"               E(M) = {EM:.2f} avg(mi) = {np.mean(cascade[1:,1]):.2f}\")\n",
    "print(f\"                  n = {len(cascade)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = 50\n",
    "pred = prediction((p,beta), cascade, alpha, mu,tmin * 60)\n",
    "print(f\"Prediction at {tmin} min = {int(pred)}\")\n",
    "\n",
    "preds = predictions((p,beta), cascade, alpha, mu)\n",
    "plt.plot(preds[:,0]/60,preds[:,1], label='pred')\n",
    "N = counting_process(cascade)\n",
    "plt.plot(N[:,0]/60, N[:,1], label='N')\n",
    "plt.plot(cascade[:,0]/60, np.zeros(len(cascade)),'o', color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even when knowing perflectly the parameters of a generative model, prediction is a very hard problem, especially for short observation windows.\n",
    "This is a general fact in prediction problems where given some past & present, there is generally a huge number of possible and strongly different futures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gathered all the basic ingredients to simulate, plot and predict Hawkes processes, time has come to look at parameter estimation.\n",
    "\n",
    "\n",
    "# Computing MLE\n",
    "\n",
    "The roadmap is to compute the *likelihood* $\\mathcal{L}(\\theta)$ of a process given the available observations in some time window $[0,t]$ and then to compute the *maximum likelihood estimator (MLE)*, i.e parameters $\\hat{\\theta}_{MLE}$ that maximize likelihood using some optimization algorithm (gradient ascent, Newton-Raphson, etc).\n",
    "\n",
    "##  Computing likelihood\n",
    "\n",
    "The first step is to compute the likelihood.\n",
    "During the lectures, we showed that the likelihood of an unmarked point process with respect to its history $\\mathcal{H}_t = \\{ (t_i)_{1 \\leq i \\leq n} \\, | \\, t_i \\leq t\\}$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta)  = P(\\mathcal{H}_t \\, | \\, \\theta) = \\left( \\prod_{i=1}^n \\lambda^*(t_i) \\right) \\, \\exp(-\\Lambda(t))$$\n",
    "However here we choose always the time reference so that $t_1 = 0$. Consequently here\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta)  = P(\\mathcal{H}_t \\, | \\, \\theta) \n",
    " = \\left( \\prod_{i=2}^n \\lambda^*(t_i) \\right) \\, \\exp(-\\Lambda(t))\n",
    "$$\n",
    "\n",
    "Now let's consider our problem that includes marks:\n",
    "given history $\\mathcal{H}_t = \\{(t_i,m_i)_{1 \\leq i \\leq n} \\, | \\, t_i \\leq t\\}$, and given the fact $\\alpha$ is a predetermined constant, the likelihood $\\mathcal{L}_m$ can be decomposed in two independent factors:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_m(\\theta) & = P(\\mathcal{H}_t \\, | \\, \\theta, \\alpha) \\\\\n",
    "& = P((t_i)_{1 \\leq i \\leq n} \\, | \\, \\theta, \\alpha) \\times P((m_i)_{1 \\leq i \\leq n} \\, | \\, (t_i)_{1 \\leq i \\leq n} \\, \\theta, \\alpha) \\\\\n",
    "& = P((t_i)_{1 \\leq i \\leq n} \\, | \\, \\theta) \\times \\prod_{i=1}^n P(m_i \\, | \\, \\alpha) \\\\\n",
    "& = \\mathcal{L}_u(\\theta) \\times cst.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We see that observed magnitudes $m_i$ have no influence on the choice of parameters $\\theta$ so that we can consider that the likelihood is just $\\mathcal{L}(\\theta) = \\mathcal{L}_u(\\theta)$.\n",
    "\n",
    "> **Question:**  \n",
    "> Simplify the expression of the loglikelihood in case of a Hawkes process with exponential kernel parameterized by $\\theta = (p,\\beta)$ and show this is equal to\n",
    "> $$\\log\\mathcal{L}(\\theta) = (n-1) \\, \\log(p\\, \\beta) + \\sum_{i=2}^n \\log\\left(\\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right)\\right) - p \\,\\left(\\sum_{i=1}^n m_i \\, \\left(1 - \\exp \\left(-\\beta(t - t_i)\\right)\\right)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Implement this expression in function ``naive_loglikelihood``. What is the time complexity with respect to $n$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_loglikelihood(params, history, t):\n",
    "    \"\"\"\n",
    "    Returns the loglikelihood of a Hawkes process with exponential kernel\n",
    "        \n",
    "    params   -- parameter tuple (p,beta) of the Hawkes process\n",
    "    history  -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    t        -- current time (i.e end of observation window)\n",
    "    \"\"\"\n",
    "    \n",
    "    p,beta = params    \n",
    "    n = len(history)\n",
    "    tis = history[:,0]\n",
    "    mis = history[:,1]\n",
    "    \n",
    "    LL = (n-1) * np.log(p * beta)\n",
    "    \n",
    "    for i in range(1,n):      \n",
    "        LL += np.log(np.sum(mis[:i] * np.exp(-beta * (tis[i] - tis[:i]))))\n",
    "        \n",
    "    LL -= p * np.sum(mis * (1. - np.exp(-beta * (t - tis))))\n",
    "\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this function behaves as expected, we can test it on a large cascade (why do we need a large cascade?). \n",
    "We provide function ``evaluate_likelihood_of_params`` for this purpose.\n",
    "Understand what it does and use it on a large cascade. \n",
    "Check you obtain the expected results.\n",
    "\n",
    "*Be careful: likelihood is here a density, not a probability (i.e might be greater than 1)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_likelihood_of_params(params, cascade, t, ratio = 0.7):\n",
    "    n = len(cascade)\n",
    "    print(f\"Number of samples = {n}\")\n",
    "    print(\"Average likelihoods per sample:\")\n",
    "    def test_params(p):\n",
    "        LL = naive_loglikelihood(p, cascade, t)\n",
    "        print(f\"  {np.exp(LL /n):0.7f} for (p,beta)=({p[0]:0.3f},{p[1]:0.5f})\")\n",
    "    \n",
    "    test_params(params)\n",
    "    test_params((params[0] * ratio, params[1]))\n",
    "    test_params((params[0] / ratio, params[1]))\n",
    "    test_params((params[0], params[1] * ratio))\n",
    "    test_params((params[0], params[1] / ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.025, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "cascade = simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu)\n",
    "print(f\"n = {len(cascade)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cascade[-1,0]\n",
    "n = len(cascade)\n",
    "\n",
    "evaluate_likelihood_of_params((p,beta), cascade, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation issues\n",
    "\n",
    "Now let's evaluate how costly it is to compute likelihood. To do so we use a provided generic test function ``evaluate_processing_time`` to measure processing times for various number of samples.\n",
    "\n",
    "> **Question:**  \n",
    "> Evaluate the algorithm using the provided code snipet and then plot the processing time versus the number of samples to determine the empirical complexity and compare it with the theoretical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def evaluate_processing_time(target, preprocessing, sizes):\n",
    "    \"\"\"\n",
    "    Returns a dataframe containing the processing time to evaluate a function for varying input size\n",
    "    \n",
    "    target        -- function to evaluate, taking as inputs the output of the preprocessing function\n",
    "    preprocessing -- function that generates inputs from a given size\n",
    "    sizes         -- range of sizes for which target functio has to be evaluated\n",
    "    \"\"\"\n",
    "    \n",
    "    times = np.zeros((len(sizes),2))\n",
    "    \n",
    "    for i,n in enumerate(sizes):\n",
    "        \n",
    "        print(f\"Preprocessing size {n}...\")\n",
    "        inputs = preprocessing(n)\n",
    "        \n",
    "        print(f\"Test size {n}\")        \n",
    "        test = timeit.Timer(lambda : target(inputs))\n",
    "        times[i] = [n, np.mean(test.timeit(1))]\n",
    "\n",
    "    return pd.DataFrame(times, columns=['size','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.04, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000\n",
    "n_star = p * mu * (alpha - 1) / (alpha - 2)\n",
    "print(f\"n_star = {n_star:.2f}\")\n",
    "if n_star < 1:\n",
    "    raise Exception(\"Branching factor must be greater than 1 for this test\")\n",
    "   \n",
    "preprocessing_function = lambda n : simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu, max_size=n)\n",
    "function_to_evaluate = lambda cascade : naive_loglikelihood(params, cascade, cascade[-1,0])\n",
    "sizes = [ 100,1000,10000,50000]\n",
    "\n",
    "perf_naive = evaluate_processing_time(function_to_evaluate, preprocessing_function, sizes)\n",
    "perf_naive.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_naive.plot('size', 'time', logx=True, logy=True, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast computation of likelihood\n",
    "\n",
    "We now want to leverage the specific properties of the exponential kernel to derive a faster algorithm to compute loglikelihood.\n",
    "$$\\log\\mathcal{L}(\\theta) = (n-1) \\, \\log(p\\, \\beta) + \\sum_{i=2}^n \\log\\left(\\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right)\\right) - p \\,\\left(\\sum_{i=1}^n m_i \\, \\left(1 - \\exp \\left(-\\beta(t - t_i)\\right)\\right)\\right) $$\n",
    "\n",
    "To do so, observe the quadratic complexity comes from the term\n",
    "$$\\sum_{i=2}^n \\log\\left(\\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right)\\right)$$\n",
    "\n",
    "> **Question:**  \n",
    "> Introducing the sequence $(A_i)$ defined as\n",
    "> $$\n",
    "\\begin{align}\n",
    "& A_0 = 0 \\\\\n",
    "\\forall i \\in \\{1 \\dots n\\}, & A_i = \\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right) \\\\\n",
    "& A_{n+1} = \\sum_{j=1}^{n} m_j \\, \\exp\\left(-\\beta \\,(t-t_j)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "> find a recursive formula that computes $A_i$ from $A_{i-1}$ in constant time, assuming $A_0 = 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "A_i & = \\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right) \\\\\n",
    "& = \\exp\\left(-\\beta \\,(t_i-t_{i-1})\\right) \\, \\left(m_{i-1} + \\sum_{j=1}^{i-2} m_j \\, \\exp\\left(-\\beta \\,(t_{i-1}-t_j)\\right) \\right) \\\\\n",
    "& = \\exp\\left(-\\beta \\,(t_i-t_{i-1})\\right) \\, \\left(m_{i-1} + A_{i-1} \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Rewriting loglikelihood as function of terms $(A_i)_{1\\leq i \\leq n+1}$, implement in function ``loglikelihood`` a algorithm that computes with a linear complexity thanks to the recursive relation the loglikelihood $\\log\\mathcal{L}(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log\\mathcal{L}(\\theta) = (n-1) \\, \\log(p\\, \\beta) + \\sum_{i=2}^n \\log\\left(A_i\\right) - p \\,\\left(\\sum_{i=1}^n m_i - A_{n+1} \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(params, history, t):\n",
    "    \"\"\"\n",
    "    Returns the loglikelihood of a Hawkes process with exponential kernel\n",
    "    computed with a linear time complexity\n",
    "        \n",
    "    params   -- parameter tuple (p,beta) of the Hawkes process\n",
    "    history  -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    t        -- current time (i.e end of observation window)\n",
    "    \"\"\"\n",
    "    \n",
    "    p,beta = params    \n",
    "    \n",
    "    if p <= 0 or p >= 1 or beta <= 0.: return -np.inf\n",
    "\n",
    "    n = len(history)\n",
    "    tis = history[:,0]\n",
    "    mis = history[:,1]\n",
    "    \n",
    "    LL = (n-1) * np.log(p * beta)\n",
    "    logA = -np.inf\n",
    "    prev_ti, prev_mi = history[0]\n",
    "    \n",
    "    i = 0\n",
    "    for ti,mi in history[1:]:\n",
    "        if(prev_mi + np.exp(logA) <= 0):\n",
    "            print(\"Bad value\", prev_mi + np.exp(logA))\n",
    "        \n",
    "        logA = np.log(prev_mi + np.exp(logA)) - beta * (ti - prev_ti)\n",
    "        LL += logA\n",
    "        prev_ti,prev_mi = ti,mi\n",
    "        i += 1\n",
    "        \n",
    "    logA = np.log(prev_mi + np.exp(logA)) - beta * (t - prev_ti)\n",
    "    LL -= p * (np.sum(mis) - np.exp(logA))\n",
    "\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Test function ``loglikelihood`` by comparing the result with the one returned by ``naive_loglikelihood``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.04, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000\n",
    "cascade = simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu)\n",
    "t = cascade[-1,0]\n",
    "n = len(cascade)\n",
    "\n",
    "LL1 = naive_loglikelihood(params, cascade, t)\n",
    "LL2 = loglikelihood(params, cascade, t)\n",
    "\n",
    "print(f\"quadratic loglikelihood per sample = {LL1/n}\")\n",
    "print(f\"   linear loglikelihood per sample = {LL2/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Compare the empirical complexity of ``loglikelihood`` and ``naive_loglikelihood``.  \n",
    "> Verify that the complexity of ``loglikelihood`` is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.04, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000\n",
    "n_star = p * mu * (alpha - 1) / (alpha - 2)\n",
    "print(f\"n_star = {n_star:.2f}\")\n",
    "if n_star < 1:\n",
    "    raise Exception(\"Branching factor must be greater than 1 for this test\")\n",
    "\n",
    "preprocessing_function = lambda n : simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu, max_size=n)\n",
    "function_to_evaluate = lambda cascade : loglikelihood(params, cascade, cascade[-1,0])\n",
    "sizes = [ 100,1000,10000,50000]\n",
    "\n",
    "perf_optim = evaluate_processing_time(function_to_evaluate, preprocessing_function, sizes)\n",
    "perf_optim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = perf_naive.plot('size', 'time', logx=True, logy=True, grid=True)\n",
    "perf_optim.plot('size', 'time', logx=True, logy=True, grid=True, ax=ax)\n",
    "plt.legend(['naive', 'optim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing MLE\n",
    "\n",
    "Given a cascade and its history $\\mathcal{H}_t$ up to time $t$ ($t$ is the lengtho of the observation window), we now search for parameters $\\hat{\\theta}_{MLE} = (\\hat{p}_{MLE},\\hat{\\beta}_{MLE})$ that maximizes $\\log\\mathcal{L}(\\theta) = \\log P(\\mathcal{H}_t \\, | \\, \\theta)$. One necessary condition is that the gradient is zero:\n",
    "$$\\frac{\\partial \\log\\mathcal{L}}{\\partial p}(\\hat{\\theta}_{MLE}) = \\frac{\\partial \\log\\mathcal{L}}{\\partial \\beta}(\\hat{\\theta}_{MLE}) = 0$$\n",
    "where\n",
    "$$\\log\\mathcal{L}(\\theta) = (n-1) \\, \\log(p\\, \\beta) + \\sum_{i=2}^n \\log\\left(\\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right)\\right) - p \\,\\left(\\sum_{i=1}^n m_i \\, \\left(1 - \\exp \\left(-\\beta(t - t_i)\\right)\\right)\\right) $$\n",
    "The gradient is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log\\mathcal{L}}{\\partial p}(p,\\beta) & = \\frac{n-1}{p} - \\sum_{i=1}^n m_i \\, \\left(1 - \\exp \\left(-\\beta(t - t_i)\\right)\\right) \\\\\n",
    "\\frac{\\partial \\log\\mathcal{L}}{\\partial \\beta}(p,\\beta) & = \\frac{n-1}{\\beta} - \\sum_{i=2}^n \\frac{\\sum_{j=1}^{i-1} m_j \\, (t_i - t_j) \\, \\exp\\left(-\\beta \\,(t_i - t_j)\\right)}{\\sum_{j=1}^{i-1} m_j \\, \\exp\\left(-\\beta \\,(t_i-t_j)\\right)} - p \\,\\left(\\sum_{i=1}^n m_i \\, (t - t_i) \\, \\exp \\left(-\\beta(t - t_i)\\right) \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> **Question:**  \n",
    "> Check the expression of the gradient given above. Is it possible to get MLE as a closed-form expression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing MLE using an optimization method\n",
    "\n",
    "Since we do not have a closed-form solution we have to compute an approximate solution using some optimization algorithm.\n",
    "\n",
    "The optimization problem is formally\n",
    "> $\\mathtt{Given~} \\mathcal{H}_t, \\, \\mathtt{minimize} \\log\\mathcal{L}_t\\left((p,\\beta)\\right) \\, \\mathtt{~wih~} p \\in \\left]0, \\frac{1}{\\mathbf{E}(M)}\\right[ \\text{ and }\\beta \\in ]0, +\\infty[$\n",
    "\n",
    "This is a multivariate (i.e defined in $\\mathbb{R}^2$) optimization problem with bounds (i.e domains of $p$ and $\\beta$ are not $\\mathbb{R}$ but restricted subsets) and no additionnal constraint.\n",
    "\n",
    "Many algorithms have been developped in different optimization libraries to solve multivariate optimization problems, with our without constraints, with or without specific shapes for the target function and/or constraints, etc. Authors of the CIKM16 paper used library ``IPOPT`` on power-law kernels with four bounded parameters and an additional constraint. We will use simply the standard ``scipy.optimize`` library as our optimization problem is relatively elementary and with a low dimension of two.\n",
    "\n",
    "In general for high dimensional problems (constrained our not), one has to resort to \"local optimization techniques\" using either first-order derivation (i.e gradient of target function and Jacobian of constraints) or sometimes second-order derivatives (i.e Hessian matrix of the target function). These techniques can be seen as improvements of the gradient ascent/descent (first order) or Newton-Raphson (second order) algorithms, that integrate additional constraints and bounds.\n",
    "\n",
    "The normal roadmap would thus be to derive the expression of the gradient and even the Hessian matrix of our likelihood function and to use one of these algorithms. \n",
    "However this would require some times to derive these expression and to find an optimized algorithm to compute it as we did for the likelihood function.\n",
    "\n",
    "Note also that nowadays, the best option is to leverage deep learning librairies (like TensorFlow, PyTorch, etc) to automatically derive the likelihood gradient and to find a local optimum. However we won't go for this option as you will study later these frameworks during the Deep Learning course.\n",
    "\n",
    "Since these derivations are error-prone, we propose to avoid this tedious work as we work on a very low dimensional space of the couple $(p,\\beta)$. In such case (and only in this case), we can use methods that only need to evaluate the target function to optimize (here the loglikelihood) without computing derivatives.\n",
    "\n",
    "Methods of the library ``scipy.optimize`` are listed in the documentation page of function\n",
    "[optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize).\n",
    "Among them, two methods are proposed that do not require derivatives:\n",
    "* [Nelder-Mead's method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) shrinks an elementary volume (i.e a simplex) towards a local optimum.\n",
    "* [Powell's method](https://en.wikipedia.org/wiki/Powell%27s_method) finds iteratively the local optimum by moving along some frame axes and then by updating these frame axes by inserting the best direction found at every step.\n",
    "\n",
    "In ``scipy.optimize``, Powell's method accepts bounds on variables when Nelder-Mead's method does not.\n",
    "We therefore choose Powell's method.\n",
    "\n",
    "> **Question:**  \n",
    "> Complete function ``compute_MLE`` to find MLE using Powell's method \n",
    "> after reading the documentation of [scipy.optimize.minimize(method=‚ÄôPowell‚Äô)](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html)  \n",
    "\n",
    "> **Note:** *Do not forget that we want to MAXIMIZE loglikelihood where the standard assumption of optimization libraries is to MINIMIZE functions.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optim\n",
    "\n",
    "def compute_MLE(history, t, alpha, mu,\n",
    "                init_params=np.array([0.0001, 1./60]), \n",
    "                max_n_star = 1., display=False):\n",
    "    \"\"\"\n",
    "    Returns the pair of the estimated loglikelihood and parameters (as a numpy array)\n",
    "\n",
    "    history     -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    t           -- current time (i.e end of observation window)\n",
    "    alpha       -- power parameter of the power-law mark distribution\n",
    "    mu          -- min value parameter of the power-law mark distribution\n",
    "    init_params -- initial values for the parameters (p,beta)\n",
    "    max_n_star  -- maximum authorized value of the branching factor (defines the upper bound of p)\n",
    "    display     -- verbose flag to display optimization iterations (see 'disp' options of optim.optimize)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the target function to minimize as minus the loglikelihood\n",
    "    target = lambda params : -loglikelihood(params, history, t)\n",
    "    \n",
    "    EM = mu * (alpha - 1) / (alpha - 2)\n",
    "    eps = 1.E-8\n",
    "\n",
    "    # Set realistic bounds on p and beta\n",
    "    p_min, p_max       = eps, max_n_star/EM - eps\n",
    "    beta_min, beta_max = 1/(3600. * 24 * 10), 1/(60. * 1)\n",
    "    \n",
    "    \n",
    "    # Define the bounds on p (first column) and beta (second column)\n",
    "    bounds = optim.Bounds(\n",
    "        np.array([p_min, beta_min]),\n",
    "        np.array([p_max, beta_max])\n",
    "    )\n",
    "    \n",
    "    # Run the optimization\n",
    "    res = optim.minimize(\n",
    "        target, init_params,\n",
    "        method='Powell',\n",
    "        bounds=bounds,\n",
    "        options={'xtol': 1e-8, 'disp': display}\n",
    "    )\n",
    "    \n",
    "    # Returns the loglikelihood and found parameters\n",
    "    return(-res.fun, res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Test ``compute_MLE`` by generating a sufficiently long cascade from a known parameterized model, compute the likelihood of the cascade and compare with the returned MLE likelihood and parameters.\n",
    "> Observe what happens when the number of samples is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_new_cascade(m0, p, beta, alpha, mu):\n",
    "    n_star = p * mu * (alpha - 1) / (alpha - 2)\n",
    "    if n_star < 1:\n",
    "        cascade = simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu)\n",
    "        n = len(cascade)\n",
    "        print(f\"         n = {n}\")\n",
    "        t = cascade[-1,0]\n",
    "\n",
    "        LL = loglikelihood((p, beta), cascade, t)\n",
    "        LL_MLE,MLE = compute_MLE(cascade, t, alpha, mu)\n",
    "        p_MLE, beta_MLE = MLE\n",
    "        n_star_MLE = p_MLE * mu * (alpha - 1) / (alpha - 2)  \n",
    "        rel_error = (np.abs(p_MLE - p) / p + np.abs(beta_MLE - beta) / beta)/2.\n",
    "\n",
    "        print(f\"        LL = {LL/n:.4f} MLE = {LL_MLE/n:.4f}\")\n",
    "        print(f\"         p = {p:.5f} MLE = {p_MLE:.5f}\")\n",
    "        print(f\"      beta = {beta:.5f} MLE = {beta_MLE:.5f}\")\n",
    "        print(f\"    n_star = {n_star:.5f} MLE = {n_star_MLE:.5f}\")\n",
    "        print(f\"rel. error = {rel_error:.2f}\")\n",
    "        return cascade\n",
    "    else:\n",
    "        print(f\"** Failure: n_star = {n_star:.2f} greater than 1 **\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.02, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000        \n",
    "cascade = get_new_cascade(m0, p, beta, alpha, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine parameter estimation with prediction.\n",
    "\n",
    "> **Question:**  \n",
    "> Using function ``predictions_from_estimator`` that makes prediction according to some estimators like MLE, plots the estimated prediction according to MLE along with the estimated prediction using the true parameter values and the counting process. What do you observe in term of variance and computation time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_from_estimator(estimator, history, alpha, mu, T=None, n_tries=1, slider=True):\n",
    "    \"\"\"\n",
    "    Compute the provided estimator for different observation windows and apply prediction according to it. Returns\n",
    "    * the expected total numbers of points for a set of time points as a 1D-array\n",
    "    * the computed loglikelihoods as a 1D-array\n",
    "    * the estimated parameters as a 2D-array\n",
    "    \n",
    "    estimator -- function that implements an estimator that expect the same arguments as compute_MLE\n",
    "    history   -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    alpha     -- power parameter of the power-law mark distribution\n",
    "    mu        -- min value parameter of the power-law mark distribution\n",
    "    T         -- 1D-array of times (i.e ends of observation window)\n",
    "    n_tries   -- number of times the estimator is run. Best result is kept.\n",
    "    \"\"\"\n",
    "\n",
    "    tis = history[:,0]\n",
    "    if T is None:\n",
    "        T = np.linspace(60, tis[-1], 50) # Compute 50 points from 1min to last time point\n",
    "    \n",
    "    N      = np.zeros((len(T),2)); N[:,0] = T\n",
    "    LLs    = np.zeros((len(T),2)); LLs[:,0] = T\n",
    "    params = np.zeros((len(T),3)); params[:,0] = T\n",
    "\n",
    "    if slider:\n",
    "        iter = tqdm(T)\n",
    "    else:\n",
    "        iter = T\n",
    "    for i,t in enumerate(iter):\n",
    "        partial_history = history[tis < t]\n",
    "        best_LL, best_params, best_N_tot = -np.inf, None, np.inf\n",
    "        for j in range(n_tries):\n",
    "            try:\n",
    "                LL, param = estimator(partial_history, t, alpha, mu)\n",
    "                if LL > best_LL:\n",
    "                    N_tot = prediction(param, partial_history, alpha, mu, t)\n",
    "                    best_LL, best_params, best_N_tot = LL, param, N_tot\n",
    "            except:\n",
    "                pass\n",
    "        N[i,1], LLs[i,1], params[i,1:] = best_N_tot, best_LL, best_params\n",
    "    return N, LLs, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_predictions(estimator, params, cascade, alpha, mu):\n",
    "    # Compute the predictions according to the estimator\n",
    "    est_preds, est_LLs, est_params = predictions_from_estimator(estimator, cascade, alpha, mu)\n",
    "    \n",
    "    # Compute the predictions according to the true parameters\n",
    "    preds = predictions(params, cascade, alpha, mu)\n",
    "    \n",
    "    # Compute the counting process\n",
    "    N = counting_process(cascade)\n",
    "\n",
    "    plt.plot(est_preds[:,0]/60, est_preds[:,1], label='est')\n",
    "    plt.plot(preds[:,0]/60, preds[:,1], label='pred')\n",
    "    plt.plot(N[:,0]/60, N[:,1], label='N')\n",
    "    plt.plot(cascade[:,0]/60, np.zeros(len(cascade)),'o', color='red')\n",
    "    plt.ylim([0,N[-1,1]*3])\n",
    "    plt.legend()    \n",
    "    \n",
    "    return est_preds, est_LLs, est_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE_preds, MLE_LLs, MLE_params = plot_predictions(compute_MLE, (p,beta), cascade, alpha, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**  \n",
    "> Plot on three different subplots (using ``plt.subplots``)\n",
    "> - the MLE loglikelihoods along with the true loglikelihoods as functions of time (i.e end of the observation window)\n",
    "> - same for the MLE parameter $p$ along with the true one\n",
    "> - same for the MLE parameter $\\beta$ along with the true one  \n",
    "> Check the estimates converge to their true values for large observation windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_estimates(est_LLs, est_params, cascade):\n",
    "    T = est_LLs[:,0]\n",
    "\n",
    "    # First we need to compute loglikelihoods for the true parameters\n",
    "    LLs = np.empty_like(T)\n",
    "    n_samples = np.empty_like(T)\n",
    "    tis = cascade[:,0]\n",
    "\n",
    "    for i,t in enumerate(tqdm(T)):\n",
    "        partial_cascade = cascade[tis < t]\n",
    "        LLs[i] = loglikelihood((p, beta), partial_cascade, t) \n",
    "        n_samples[i] = len(partial_cascade)\n",
    "\n",
    "\n",
    "    # Then we can draw the subplots\n",
    "    _, axis = plt.subplots(3)\n",
    "    \n",
    "    axis[0].plot(T/60, est_LLs[:,1] / n_samples, label='$log\\mathcal{L}_{est}/sample$')\n",
    "    axis[0].plot(T/60, LLs / n_samples, label='$log\\mathcal{L}_{true}/sample$')\n",
    "    axis[0].legend() \n",
    "\n",
    "    axis[1].plot(T/60, est_params[:,1], label='$p_{est}$')\n",
    "    axis[1].plot(T/60, p * np.ones_like(T), label='$p_{true}$')\n",
    "    axis[1].legend() \n",
    "\n",
    "    axis[2].plot(T/60, est_params[:,2], label='$beta_{est}$')\n",
    "    axis[2].plot(T/60, beta * np.ones_like(T), label='$beta_{true}$')\n",
    "    axis[2].set_ylim((0,0.002))\n",
    "    axis[2].legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_estimates(MLE_LLs, MLE_params, cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map estimator\n",
    "\n",
    "We can try to use the Maximum A Posteriori estimator (MAP) instead of MLE.\n",
    "MAP is the mode of the A Posteriori distribution, assuming some prior. It is a regularized version of MLE.\n",
    "\n",
    "## Choosing a prior\n",
    "\n",
    "We should choose our prior to reflect our knowledge about the initial distribution of parameters $(p,\\beta)$.\n",
    "However in our case, we have no idea about what could be a good prior for tweet cascades. We will therefore use an approach that is sometimes called **empirical Bayes** consisting in estimating the prior from the data.\n",
    "\n",
    "The idea is first to collect a collection of good and typical set of parameters $(p_i,\\beta_i)$ from some set of observed cascades. More precisely for each observed cascade:\n",
    "* We evaluate its parameters by using the MLE estimator applied on the whole cascade. \n",
    "* We then use these parameters to predict the final size of the cascade for some observation window. \n",
    "* If the final size is too faraway from the true size, we reject this cascade (this is to prevent to include odd cascades that are too hard to predict).\n",
    "* Otherwise we add the parameters to the collection. \n",
    "\n",
    "In a second step, once we have extracted the set $(p_i,\\beta_i)$,\n",
    "* We observe the empirical distribution of the set $(p_i,\\beta_i)$,\n",
    "* We choose a parametric distribution for our prior that matches the shape of the empirical distribution.\n",
    "* Finally we estimate the parameters of our prior that better matches the empirical distribution.\n",
    "\n",
    "Here we will skip some steps and we will decide somewhat arbitrarily the type of prior distribution:\n",
    "Since $p$ and $\\beta$ are positive, one simple but still realistic choice of prior is to assume $(p,\\beta)$ follows a *log-normal distribution*.\n",
    "Put it another way, it amounts to assume $\\log \\theta = (\\log(p), \\log(\\beta))$ follows a multivariate normal distribution (MVN) of parameter $(\\mu, \\Sigma)$ where $\\mu$ is the expected vector and $\\Sigma$ the covariance matrix of $\\log \\theta$.\n",
    "\n",
    "> **Question:**  \n",
    "> Write the expression of the aposteriori log density as a function of the loglikelihood (we won't develop the expression) and prior parameters $\\mu$ and $\\Sigma$. What interpretation can we give to the term linked to the prior?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Since the aposteriori density is the product of the apriori density and the likelihood,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log f(\\theta | \\mathcal{H}_t, \\mu, \\Sigma) & = \\log f(\\theta | \\mu, \\Sigma) + \\log\\mathcal{L}_t\\left(\\theta\\right) \\text{ with } \\theta = [ p,\\beta ]^T \\\\\n",
    "& = cst + -\\frac{1}{2} \\, (\\log \\theta - \\mu)^T \\, \\Sigma^{-1} \\, (\\log \\theta - \\mu) + \\log\\mathcal{L}_t\\left(\\theta\\right) \\end{align}\n",
    "$$\n",
    "\n",
    "The prior term $(\\log \\theta - \\mu)^T \\, \\Sigma^{-1} \\, (\\log \\theta - \\mu)$ behaves like a classical $L2$ penalization term on $\\log \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of moments\n",
    "\n",
    "We thus need to estimate the prior parameters $(\\mu, \\Sigma)$ from  a set of pairs $(p_i,\\beta_i)$.\n",
    "The simplest solution here would be to transform these pairs into $(\\log(p_i),\\log(\\beta_i))$ and estimate from this new collection the sample mean $\\mu$ and covariance $\\Sigma$. This is possible because MVN is parameterized by its first two moments.\n",
    "\n",
    "However in the general case this wouldn't work and we would need to derive an estimator for our prior parameters. In simple cases like this one, one will typically use a simple method called **method of moments**.\n",
    "It consists in identifying the empirical moments observed in the data with the theoretical moments given by the parametric distribution we choosed, and isolate the parameters from these set of equations.\n",
    "\n",
    "While not necessary here, we apply the method of moments as an exercise.\n",
    "Given variables $(X_1, \\dots, X_m)$ following a MVN of parameters $(\\mu, \\Sigma)$, $\\forall i, Y_i = \\exp(X_i)$,\n",
    "we know that\n",
    "- $\\mathbb{E}(Y_i) = \\exp\\left(\\mu_i +\\frac{1}{2} \\Sigma_{ii}\\right)$\n",
    "- $cov(Y_i,Y_j) = \\exp\\left(\\mu_i + \\mu_j +\\frac{1}{2} (\\Sigma_{ii} + \\Sigma_{jj})\\right) \\, \\left(\\exp(\\Sigma_{ij})-1\\right)$\n",
    "\n",
    "Suppose we can estimate from some data the two first moments $\\mathbb{E}(Y_i)$ and $cov(Y_i,Y_j)$ of variables $Y_i$. Inverting the equations above lead to estimates for the parameters $(\\mu, \\Sigma)$.\n",
    "\n",
    "> **Question:**  \n",
    "> Find the expression of $\\mu$ and $\\Sigma$ from the sample averages $\\bar{p}$, $\\bar{\\beta}$, variances $q_p$, $q_{\\beta}$ and covariance $q_{p,\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Applying the formulas to our problem, we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar{p} & = \\exp\\left(\\mu_1 +\\frac{1}{2} \\Sigma_{11}\\right)\\\\\n",
    "\\bar{\\beta} & = \\exp\\left(\\mu_2 +\\frac{1}{2} \\Sigma_{22}\\right)\\\\\n",
    "q_{p} & = \\exp\\left(2 \\, \\mu_1 + \\Sigma_{11}\\right) \\, \\left(\\exp(\\Sigma_{11})-1\\right) = \\bar{p}^2 \\, \\left(\\exp(\\Sigma_{11})-1\\right) \\\\\n",
    "q_{\\beta} & = \\exp\\left(2 \\, \\mu_2 + \\Sigma_{22}\\right) \\, \\left(\\exp(\\Sigma_{22})-1\\right) = \\bar{\\beta}^2 \\, \\left(\\exp(\\Sigma_{22})-1\\right) \\\\\n",
    "q_{p,\\beta} & = \\exp\\left(\\mu_1 + \\mu_2 +\\frac{1}{2} (\\Sigma_{11} + \\Sigma_{22})\\right) \\, \\left(\\exp(\\Sigma_{12})-1\\right) = \\bar{p} \\, \\bar{\\beta} \\, \\left(\\exp(\\Sigma_{12})-1\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\n",
    "\\Sigma = \\log\\left( \n",
    "\\begin{bmatrix}\n",
    "\\frac{q_{p}}{\\bar{p}^2} + 1 & \\quad \\frac{q_{p, \\beta}}{\\bar{p}\\, \\bar{\\beta}} + 1 \\\\\n",
    "\\frac{q_{p, \\beta}}{\\bar{p}\\, \\bar{\\beta}} + 1 & \\quad \\frac{q_{\\beta}}{\\bar{\\beta}^2} + 1\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "\\quad\n",
    "\\mu = \\log\\left(\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\bar{p}^2}{\\sqrt{q_{p} + \\bar{p}^2}} \\\\\n",
    "\\frac{\\bar{\\beta}^2}{\\sqrt{q_{\\beta} + \\bar{\\beta}^2}}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the covariance matrix $Q$ can be rewritten as a function of more convenient measures:\n",
    "$$\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "q_p & q_{p,\\beta} \\\\\n",
    "q_{p,\\beta} & q_{\\beta}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sigma_p^2 & \\quad \\rho_{p,\\beta} \\, \\sigma_p \\, \\sigma_{\\beta} \\\\\n",
    "\\rho_{p,\\beta} \\, \\sigma_p \\, \\sigma_{\\beta} & \\sigma_{\\beta}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\sigma_p$ and $\\sigma_{\\beta}$ are resp. the standard deviations of $p$ and $\\beta$, and where $\\rho_{p,\\beta}$ is the correlation coefficient between $p$ and $\\beta$.\n",
    "\n",
    "## MAP implementation\n",
    "\n",
    "Therefore the prior can be fully specified by the list of parameters $\\bar{p}$, $\\bar{\\beta}$, $\\sigma_p$, $\\sigma_{\\beta}$, and $\\rho_{p,\\beta}$.\n",
    " \n",
    "> **Question:**  \n",
    "> Clone function ``compute_MLE`` into function ``compute_MAP`` below and adapt it to take into account a prior.\n",
    "> The parameters of the prior will be defined as arguments of the function. Since the prior mean vector is the most probable value for ou parameters a priori, this will be used as the initial value for parameters passed to the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MAP(history, t, alpha, mu,\n",
    "                prior_params = [ 0.02, 0.0002, 0.01, 0.001, -0.1],\n",
    "                max_n_star = 1, display=False):\n",
    "    \"\"\"\n",
    "    Returns the pair of the estimated logdensity of a posteriori and parameters (as a numpy array)\n",
    "\n",
    "    history      -- (n,2) numpy array containing marked time points (t_i,m_i)  \n",
    "    t            -- current time (i.e end of observation window)\n",
    "    alpha        -- power parameter of the power-law mark distribution\n",
    "    mu           -- min value parameter of the power-law mark distribution\n",
    "    prior_params -- list (mu_p, mu_beta, sig_p, sig_beta, corr) of hyper parameters of the prior\n",
    "                 -- where:\n",
    "                 --   mu_p:     is the prior mean value of p\n",
    "                 --   mu_beta:  is the prior mean value of beta\n",
    "                 --   sig_p:    is the prior standard deviation of p\n",
    "                 --   sig_beta: is the prior standard deviation of beta\n",
    "                 --   corr:     is the correlation coefficient between p and beta\n",
    "    max_n_star   -- maximum authorized value of the branching factor (defines the upper bound of p)\n",
    "    display      -- verbose flag to display optimization iterations (see 'disp' options of optim.optimize)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute prior moments\n",
    "    mu_p, mu_beta, sig_p, sig_beta, corr = prior_params\n",
    "    sample_mean = np.array([mu_p, mu_beta])\n",
    "    cov_p_beta = corr * sig_p * sig_beta\n",
    "    Q = np.array([[sig_p ** 2, cov_p_beta], [cov_p_beta, sig_beta **2]])\n",
    "    \n",
    "    # Apply method of moments\n",
    "    cov_prior = np.log(Q / sample_mean.reshape((-1,1)) / sample_mean.reshape((1,-1)) + 1)\n",
    "    mean_prior = np.log(sample_mean) - np.diag(cov_prior) / 2.\n",
    "\n",
    "    # Compute the covariance inverse (precision matrix) once for all\n",
    "    inv_cov_prior = np.asmatrix(cov_prior).I\n",
    "\n",
    "    # Define the target function to minimize as minus the log of the a posteriori density    \n",
    "    def target(params):\n",
    "        log_params = np.log(params)\n",
    "        \n",
    "        if np.any(np.isnan(log_params)):\n",
    "            return np.inf\n",
    "        else:\n",
    "            dparams = np.asmatrix(log_params - mean_prior)\n",
    "            prior_term = float(- 1/2 * dparams * inv_cov_prior * dparams.T)\n",
    "            logLL = loglikelihood(params, history, t)\n",
    "            return - (prior_term + logLL)\n",
    "      \n",
    "    EM = mu * (alpha - 1) / (alpha - 2)\n",
    "    eps = 1.E-8\n",
    "\n",
    "    # Set realistic bounds on p and beta\n",
    "    p_min, p_max       = eps, max_n_star/EM - eps\n",
    "    beta_min, beta_max = 1/(3600. * 24 * 10), 1/(60. * 1)\n",
    "    \n",
    "    # Define the bounds on p (first column) and beta (second column)\n",
    "    bounds = optim.Bounds(\n",
    "        np.array([p_min, beta_min]),\n",
    "        np.array([p_max, beta_max])\n",
    "    )\n",
    "    \n",
    "    # Run the optimization\n",
    "    res = optim.minimize(\n",
    "        target, sample_mean,\n",
    "        method='Powell',\n",
    "        bounds=bounds,\n",
    "        options={'xtol': 1e-8, 'disp': display}\n",
    "    )\n",
    "    # Returns the loglikelihood and found parameters\n",
    "    return(-res.fun, res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't used real data so far, let's test our method on the faked following values\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\bar{p}\\\\\n",
    "\\bar{\\beta}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.02\\\\\n",
    "0.0002\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "q_p & q_{p,\\beta} \\\\\n",
    "q_{p,\\beta} & q_{\\beta}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sigma_p^2 & \\quad \\rho_{p,\\beta} \\, \\sigma_p \\, \\sigma_{\\beta} \\\\\n",
    "\\rho_{p,\\beta} \\, \\sigma_p \\, \\sigma_{\\beta} & \\sigma_{\\beta}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "with standard deviations $\\sigma_p = 0.01$ and $\\sigma_{\\beta} = 0.001$ and a correlation coefficient of $\\rho_{p,\\beta} = -0.1$.\n",
    " \n",
    " > **Question:**  \n",
    " > Evaluate the MAP estimator as you did for MLE.\n",
    " > What do you observe when you compare both results, especially for early times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = [ 0.02, 0.0002, 0.01, 0.001, -0.1]\n",
    "def compute_MAP_with_prior(history, t, alpha, mu):\n",
    "    return compute_MAP(history, t, alpha, mu, prior_params=prior_params)\n",
    "\n",
    "MAP_preds, MAP_LLs, MAP_params = plot_predictions(compute_MAP_with_prior, (p,beta), cascade, alpha, mu)\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_estimates(MAP_LLs, MAP_params, cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ease comparisons, we need to draw several estimations on the same plot.\n",
    "\n",
    "> **Question:**  \n",
    "> Implement function ``compare_estimator_predictions`` and run it several times on different cascades sampled from the same set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_estimator_predictions(estimators, params, cascade, alpha, mu):\n",
    "    \"\"\"\n",
    "    Draw on the same plot the estimation from several estimators along with the true prediction\n",
    "    \n",
    "    estimators -- a dictionary whose\n",
    "               --  * keys are estimator names\n",
    "               --  * mapped values are the estimator functions with the same signature as compute_MLE\n",
    "    params     -- the true parameter tuple (p,beta) of the Hawkes process (to draw the estimation)\n",
    "    alpha     -- power parameter of the power-law mark distribution\n",
    "    mu        -- min value parameter of the power-law mark distribution\n",
    "    T         -- 1D-array of times (i.e ends of observation window)\n",
    "    n_tries   -- number of times the estimator is run. Best result is kept.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the predictions according to the true parameters\n",
    "    preds = predictions((p,beta), cascade, alpha, mu)\n",
    "    \n",
    "    # Compute the counting process\n",
    "    N = counting_process(cascade)\n",
    "\n",
    "    for name,estimator in estimators.items():\n",
    "        # Compute the predictions according to the estimator\n",
    "        est_preds, est_LLs, est_params = predictions_from_estimator(estimator, cascade, alpha, mu)\n",
    "        plt.plot(est_preds[:,0]/60, est_preds[:,1], label=name)\n",
    "        \n",
    "    plt.plot(preds[:,0]/60, preds[:,1], label='pred')\n",
    "    plt.plot(N[:,0]/60, N[:,1], label='N')\n",
    "    plt.ylim([0,N[-1,1]*3])\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, beta = 0.022, 1/3600.\n",
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000\n",
    "\n",
    "estimators = { \"MLE\": compute_MLE, \"MAP\": compute_MAP_with_prior}\n",
    "\n",
    "# Generate a new cascade\n",
    "cascade = get_new_cascade(m0, p, beta, alpha, mu)\n",
    "\n",
    "# Compute and plot the estimations\n",
    "compare_estimator_predictions(estimators, (p,beta), cascade, alpha, mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We want now to compare performances of MLE and MAP on a large number of cascades.\n",
    "Before processing real tweets, we first test estimators on simulated data.\n",
    "\n",
    "## Data generation\n",
    "\n",
    "These cascades will be drawn from Hawkes processes whose parameters are themselves drawn from our prior distribution on $(p,\\beta)$.\n",
    "\n",
    "The first step is to sample parameters of many Hawkes processes. Because *numpy* only allows to sample univariate log normal distribution (see ``np.random.lognormal``), one needs to implement our own function.\n",
    "\n",
    "> **Question:**  \n",
    "> Implement function ``multivariate_log_normal`` based on ``np.random.multivariate_normal`` (we will restrict our function for random vector of size 2). \n",
    "> Test the function by drawing histograms of $p$ and $\\beta$ along with their theoretical densities (you can use function ``plot_log_normal_density`` for this purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_log_normal(mu, cov, size=1):\n",
    "    \"\"\"\n",
    "    Returns a (size,2)-array containing iid samples of (p,beta) drawn from a given prior distribution\n",
    "\n",
    "    mu   -- mean vector of size 2\n",
    "    cov  -- covariance matrix of size 2\n",
    "    size -- number of samples\n",
    "    \"\"\"\n",
    "    \n",
    "    mu, cov = np.asarray(mu), np.asarray(cov)\n",
    "    cov_on_log  = np.log(cov / mu.reshape((-1,1)) / mu.reshape((1,-1)) + 1)\n",
    "    mean_on_log = np.log(mu) - np.diag(cov_on_log) / 2.\n",
    "\n",
    "    log_params = np.random.multivariate_normal(mean_on_log, cov_on_log, size=size)\n",
    "    return np.exp(log_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_normal_density(X, mu, var, color='r'):\n",
    "    \"\"\"\n",
    "    Plots the density of a lognormal distribution\n",
    "\n",
    "    X    -- 1D-array of values whose density has to be computed\n",
    "    mu   -- mean\n",
    "    var  -- variance\n",
    "    \"\"\"\n",
    "    \n",
    "    log_var = np.log(var / (mu ** 2) + 1)\n",
    "    log_mu = np.log(mu) - log_var / 2.     \n",
    "    plt.plot(X, np.exp(- (np.log(X) - log_mu)**2 / (2. * log_var)) / (X * np.sqrt(2 * np.pi * log_var)), color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0, cov0 = np.array([1,2]), np.array([[0.1,0.01],[0.01, 0.2]])\n",
    "\n",
    "params = multivariate_log_normal(mu0, cov0, size=10000)\n",
    "_ = plt.hist(params[:,0], bins=200, density=True, log=False, label='p')\n",
    "_ = plt.hist(params[:,1], bins=200, density=True, log=False, label='beta')\n",
    "plt.legend()\n",
    "\n",
    "# Compare with theoretical density\n",
    "X = np.linspace(0.1, 5, 400)\n",
    "plot_log_normal_density(X, mu0[0], cov0[0,0], 'b')\n",
    "plot_log_normal_density(X, mu0[1], cov0[1,1], 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since eventually we want to test our estimators on real datas that are saved into csv files, the next step is to save and load cascades from / to files. To this end we provide beneath two functions that you might want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cascade_directory = './cache/cascades'\n",
    "\n",
    "def write_cascade(cascade, casc_index, cascade_directory=default_cascade_directory):\n",
    "    \"\"\"\n",
    "    Save a cascade into a csv file\n",
    "\n",
    "    cascade           -- (n,2) numpy array containing marked time points (t_i,m_i)\n",
    "    casc_index        -- index of the cascade (int)\n",
    "    cascade_directory -- directory where the file is saved\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(cascade_directory):\n",
    "        os.makedirs(cascade_directory, exist_ok=False)\n",
    "    df = pd.DataFrame(cascade, columns=['time', 'magnitude'])\n",
    "    df.to_csv(os.path.join(cascade_directory, 'casc-{}.csv'.format(casc_index)), columns=['time', 'magnitude'])\n",
    "\n",
    "def read_cascade(casc_index, cascade_directory=default_cascade_directory):\n",
    "    \"\"\"\n",
    "    Reads a cascade csv file and returns its cascade as a numpy array containing marked time points (t_i,m_i)\n",
    "\n",
    "    casc_index        -- index of the cascade (int)\n",
    "    cascade_directory -- directory where the file is saved\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(cascade_directory, 'casc-{}.csv'.format(casc_index)), names=['time', 'magnitude'], header=0)\n",
    "    return df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read a cascade at index 0\n",
    "write_cascade(cascade, 0)\n",
    "saved_cascade = read_cascade(0)\n",
    "\n",
    "# Check there is no difference\n",
    "max_diff = np.max(np.abs(saved_cascade - cascade))\n",
    "print(f\"Maximal difference between original and saved cascades = {max_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate many cascades. \n",
    "\n",
    "> **Question:**  \n",
    "> Implement function ``generate_pseudo_data`` that generates from some prior a set of cascades and saves them on disks. And don't forget to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudo_data(n, prior_params, m0, alpha, mu):\n",
    "    \"\"\"\n",
    "    Sample cascades from some parameter prior and save them on disk.\n",
    "    \n",
    "    n            -- number of cascades to generate\n",
    "    prior_params -- list (mu_p, mu_beta, sig_p, sig_beta, corr) of hyper parameters of the prior\n",
    "    m0           -- magnitude of the initial tweet at t = 0.\n",
    "    alpha        -- power parameter of the power-law mark distribution\n",
    "    mu           -- min value parameter of the power-law mark distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate parameters from prior\n",
    "    mu_p, mu_beta, sig_p, sig_beta, corr = prior_params\n",
    "    mu_params = [mu_p, mu_beta]\n",
    "    cov_p_beta = corr * sig_p * sig_beta\n",
    "    Q = np.array([[sig_p ** 2, cov_p_beta], [cov_p_beta, sig_beta **2]])\n",
    "    params = multivariate_log_normal(mu_params, Q, size=n)\n",
    "\n",
    "    # Then generate cascades\n",
    "    for i,(p,beta) in enumerate(params):\n",
    "        cascade = simulate_marked_exp_hawkes_process((p,beta), m0, alpha, mu)\n",
    "        write_cascade(cascade, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, mu = 2.4, 10\n",
    "m0 = 1000\n",
    "generate_pseudo_data(100, prior_params, m0, alpha, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to test estimators. To evaluate the quality of prediction, we use the ean absolute error (MAE):\n",
    "$$ MAE(y,\\hat{y}) = |y - \\hat{y}|$$\n",
    "\n",
    "> **Question:**  \n",
    "> Complete function ``evaluate_ARE`` and test by plotting predictions for different time windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ARE(ids, alpha, mu, estimators, T):\n",
    "    \"\"\"\n",
    "    Compute the mean absolute error (MAE) of different estimators on a subset of your dataset.\n",
    "    Returns a 3D-array of shape (length of T, number of estimators, length of ids)\n",
    "    \n",
    "    ids        -- a list of sample indexes (e.g range(40) processes the 40 first samples)\n",
    "    alpha      -- power parameter of the power-law mark distribution\n",
    "    mu         -- min value parameter of the power-law mark distribution\n",
    "    estimators -- a dictionary whose\n",
    "               --  * keys are estimator names\n",
    "               --  * mapped values are the estimator functions with the same signature as compute_MLE\n",
    "    T          -- 1D-array of times (i.e ends of observation window)\n",
    "    \"\"\"\n",
    "\n",
    "    ARE = np.empty((len(T), len(estimators), len(ids)))\n",
    "    for i,id in enumerate(tqdm(ids)):\n",
    "        cascade = read_cascade(id)\n",
    "        N = len(cascade)\n",
    "        for j,(name,estimator) in enumerate(estimators.items()):\n",
    "            N_est, LLs, params = predictions_from_estimator(estimator, cascade, alpha, mu, T, n_tries=1, slider=False)\n",
    "            ARE[:,j,i] = np.abs(N - N_est[:,1]) / N\n",
    "\n",
    "    return ARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 60 * np.array([ 5, 15, 30, 60, 120, 240])\n",
    "ARE = evaluate_ARE(range(40), alpha, mu, estimators, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ARE_by_times(estimators, ARE):\n",
    "    n_Tobs, n_est, n_samples = ARE.shape\n",
    "    n_estimators = len(estimators)\n",
    "        # Then we can draw the subplots\n",
    "    _, axis = plt.subplots(n_Tobs)\n",
    "    \n",
    "    for i, ax in enumerate(axis):\n",
    "        for j,(name,estimator) in enumerate(estimators.items()):\n",
    "            ax.semilogy(ARE[i,j], label=name)\n",
    "        ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ARE_by_times(estimators, ARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "\n",
    "def plot_ARE_by_estimators(estimators, ARE, Tobs):\n",
    "    n_Tobs, n_est, n_samples = ARE.shape\n",
    "    n_estimators = len(estimators)\n",
    "        # Then we can draw the subplots\n",
    "    _, axis = plt.subplots(n_estimators)\n",
    "    \n",
    "    for i,(name,estimator) in enumerate(estimators.items()):\n",
    "        for j, t in enumerate(Tobs):\n",
    "            axis[i].plot(ARE[j,i], label=str(t))\n",
    "            axis[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ARE_by_estimators(estimators, ARE, T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
